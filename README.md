# Artificial Intelligence Fundamentals labs

## 1. Градиентный спуск и его модификации

- [x] Выбрать 2x тестовые функции оптимизации
- [x] Запрограммировать собственную реализацию классического градиентного спуска
- [x] Запрограммировать пайлайн тестирования алгоритма оптимизации
  - [x] Визуализации функции и точки оптимума
  - [x] Вычисление погрешности найденного решения в сравнение с аналитическим для нескольких запусков
  - [x] Визуализации точки найденного решения
- [x] Запрограммировать метод вычисления градиента
  - [x] Передача функции градиента от пользователя
  - [ ] Символьное вычисление градиента например с помощью `sympy`
  - [ ] Численная аппроксимация градиента
- [x] Запрограммировать одну моментную модификацию и протестировать ее
- [x] Запрограммировать одну адаптивную модификацию и протестировать ее
- [x] Запрограммировать метод эфолюции темпа обучения и/или метод выбора начального приближения и протестировать их

## 2. Глобальная оптимизация и метаэврестические алгоритмы

- [x] В Pygmo запрограммировать две своих тестовых функции
- [x] Найти их оптимум 3 разными алгоритмами доступными в библиотеке
- [x] Получить таблицу сравнения

## 3. Оптимизация гиперпараметров

- [x] С помощью optuna взять пример, аналогичный третьему туториалу документации,
используя sklearn и с другим датасетом, выбрать другие алгоритмы классификации и
кластеризации не из туториала
- [x] Визуализировать графики для полученного процесса
- [x] Использовать 2 разных семплера и прунера
- [x] При процессе оптимизации гиперпараметров использовать общую память используя PostgreSQL

![Plot 1](./lab-3-plot1.png)

![Plot 2](./lab-3-plot2.png)

![Plot 3](./lab-3-plot3.png)

![Plot 4](./lab-3-plot4.png)

В качестве других моделей подойдут любые алгоритмы классификации и регрессии
из sklearn которые не использовались в туториале

В качестве отчёта выступают: исходный код, инструкция запуска реляционной БД

Чтобы запустить PostgreSQL для хранилища под Optuna, достаточно лишь этой команды:
```sh
docker run --name optuna-storage -p 5432:5432 -e POSTGRES_USER=optuna -e POSTGRES_PASSWORD=assword -e POSTGRES_DB=optuna_db -d postgres:17.1
```

Чтобы положить базу данных:
```sh
docker stop optuna-storage
```

## 4. Восстановление функции распренделения вероятности

- [x] Реализовать метод восстановления плотности вероятности двумя способами:
  - [x] EM-алгоритм
  - [x] Ядерное сглаживание
- [x] Применить данные методы на любом наборе случайных точек
- [x] Реализовать метод Метрополиса-Гастингса и Гибсона для несимметричного распределения.
- [x] Применить два метода на основе той функции плотности, которая была восстановлена в прошлом пункте, тем самым получив изначальные точки.
- [ ] в методе М-Г нарисовать картинку блуждания в случае 3D-функции плотности (на доп баллы)
- [x] Сравнить красный и синий набор точек (сгенерированые и исходные) (с помощью Расстояния Кульбака-Лейблера)
